name: Merolagani Scraper

on:
  schedule:
    # Run daily at 6:00 AM UTC (adjust time as needed)
    - cron: '0 6 * * *'
  workflow_dispatch:  # Allows manual triggering
    inputs:
      scrape_date:
        description: 'Date to scrape (YYYY-MM-DD format)'
        required: false
        type: string
      max_pages:
        description: 'Maximum number of pages to scrape (optional)'
        required: false
        type: string
        default: ''

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # Increase timeout to 6 hours (maximum allowed)
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 pandas pyarrow
    
    - name: Configure system for better performance
      run: |
        # Make more memory available by disabling unnecessary services
        sudo systemctl stop snapd snapd.socket snapd.seeded
        sudo systemctl disable snapd snapd.socket snapd.seeded
        
        # Display available system resources
        free -h
        nproc
        
        # Create output directories and make them writable
        mkdir -p temp_data
        mkdir -p public
        chmod -R 777 temp_data public
    
    - name: Run scraper
      run: |
        # Set environment variables to optimize Python memory usage
        export PYTHONUNBUFFERED=1
        export PYTHONMALLOC=malloc
        export MALLOC_TRIM_THRESHOLD_=65536
        
        # Prepare command with optional parameters
        CMD="python merolagani_scraper.py"
        
        # Add date parameter if provided
        if [ -n "${{ github.event.inputs.scrape_date }}" ]; then
          CMD="$CMD --date ${{ github.event.inputs.scrape_date }}"
          echo "Scraping data for date: ${{ github.event.inputs.scrape_date }}"
        else
          echo "Scraping latest data"
        fi
        
        # Add max pages parameter if provided
        if [ -n "${{ github.event.inputs.max_pages }}" ]; then
          CMD="$CMD --max-pages ${{ github.event.inputs.max_pages }}"
          echo "Maximum pages set to: ${{ github.event.inputs.max_pages }}"
        fi
        
        # Run the command
        echo "Executing: $CMD"
        $CMD 2>&1 | tee scraper_output.log
        
        # Check if scraping was successful
        if [ $? -ne 0 ]; then
          echo "Scraping failed!"
          exit 1
        fi
        
        # Print directory contents for debugging
        echo "Current directory contents:"
        ls -la
        echo "public directory contents:"
        ls -la public || echo "Directory doesn't exist"
    
    - name: Commit files to repository
      run: |
        git config --global user.name 'github-actions'
        git config --global user.email 'github-actions@github.com'
        
        # Add all files in the public directory
        git add public/
        
        # Set commit message with date info
        COMMIT_MSG="Update floorsheet data"
        if [ -n "${{ github.event.inputs.scrape_date }}" ]; then
          COMMIT_MSG="$COMMIT_MSG for ${{ github.event.inputs.scrape_date }}"
        else
          COMMIT_MSG="$COMMIT_MSG $(date +'%Y-%m-%d')"
        fi
        
        # Commit and push the changes
        git commit -m "$COMMIT_MSG" || echo "No changes to commit"
        git push
    
    - name: Upload scraped data
      uses: actions/upload-artifact@v4
      with:
        name: floorsheet-data
        path: |
          public/floorsheet.parquet
        retention-days: 7
        if-no-files-found: warn
